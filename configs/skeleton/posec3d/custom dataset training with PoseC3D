
## This documents provides a step by step approach on how to train your custom dataset with PoseC3D's skeleton based action recognition task.

First, you should know that PoseC3D action recognition requires skeleton information only and for that you need to prepare your custom annotation files (for training and validation).

- To start with, you need to replace the placeholder `mmdet_root` and `mmpose_root` in ntu_pose_extraction.py with your installation path. Then you need to take advantage of [ntu_pose_extraction.py](https://github.com/open-mmlab/mmaction2/blob/90fc8440961987b7fe3ee99109e2c633c4e30158/tools/data/skeleton/ntu_pose_extraction.py) as shown [Prepare Annotations](https://github.com/open-mmlab/mmaction2/blob/master/tools/data/skeleton/README.md#prepare-annotations) to extract 2D keypoints of each video of your custom dataset. The command looks like this (assuming the name of your video is _some_video_from_my_dataset_ ):

`python ntu_pose_extraction.py some_video_from_my_dataset.mp4 some_video_from_my_dataset.pkl`

You will be using the above command to generate pickle files for all of your train and validation videos. 

@kennymckormick's [note](https://github.com/open-mmlab/mmaction2/issues/1216#issuecomment-950130079):  

> One only thing you may need to change is that: since ntu_pose_extraction.py is developed specifically for pose extraction of NTU videos, you can skip the [ntu_det_postproc](https://github.com/open-mmlab/mmaction2/blob/90fc8440961987b7fe3ee99109e2c633c4e30158/tools/data/skeleton/ntu_pose_extraction.py#L307) step when using this script for extracting pose from your custom video datasets.

- Then, you will collect all the pickle files into one list for training (and, of course, for validation). As `ntu_pose_extraction.py` makes annotation files as exactly as shown in [The Format of PoseC3D Annotations](https://github.com/open-mmlab/mmaction2/blob/master/tools/data/skeleton/README.md#the-format-of-posec3d-annotations), you finalize preparing annotation files for your custom dataset. Now you have `custom_dataset_train.pkl` and `custom_dataset_val.pkl`.
(You may also think of a way to modify the script so that it reads all video samples in your `train` (and `validation`) folder once and make a pickle file  for each video sample)

- Next,  you may use the following script (with some alterations according to your needs) for training as shown in [PoseC3D/Train](https://github.com/open-mmlab/mmaction2/blob/master/configs/skeleton/posec3d/README.md#train):
`python tools/train.py configs/skeleton/posec3d/slowonly_r50_u48_240e_ntu120_xsub_keypoint.py --work-dir work_dirs/slowonly_r50_u48_240e_ntu120_xsub_keypoint --validate --test-best --gpus 2 --seed 0 --deterministic`

- Before running the above script, you need to modify the variables to initialize with your newly made annotation files:

    `ann_file_train = 'data/posec3d/custom_dataset_train.pkl'`
    `ann_file_val = 'data/posec3d/custom_dataset_val.pkl'`

   and `num_classes=4` (# your class categories), `pretrained=None`, (when training from scratch, 
   `pretrained='torchvision://resnet50'` when fine-tuning) and training schedule alterations as needed in 
   [slowonly_r50_u48_240e_ntu120_xsub_keypoint.py](https://github.com/open-mmlab/mmaction2/blob/master/configs/skeleton/posec3d/slowonly_r50_u48_240e_ntu120_xsub_keypoint.py)

With that, your machine should start its work to let you grab a cup of coffee and watch how the training goes.
