import copy
import os.path as osp

import numpy as np
import torch
from mmcv.runner import get_dist_info


class LFB(object):
    """Long-Term Feature Bank (LFB).

    LFB is proposed in `Long-Term Feature Banks for Detailed Video
    Understanding <https://arxiv.org/abs/1812.05038>`_

    The ROI features of videos are stored in the feature bank. The feature bank
    was generated by inferring with a lfb infer config.

    Formally, LFB is a Dict whose keys are video IDs and its values are also
    Dicts whose keys are timestamps in seconds. Here is an example.
    {
        '0f39OWEqJ24': {
            901: tensor([[ 1.2760,  1.1965,  ...,  0.0061, -0.0639],
                [-0.6320,  0.3794,  ..., -1.2768,  0.5684],
                [ 0.2535,  1.0049,  ...,  0.4906,  1.2555],
                [-0.5838,  0.8549,  ..., -2.1736,  0.4162]]),
            ...
            1705: tensor([[-1.0169, -1.1293,  ...,  0.6793, -2.0540],
                [ 1.2436, -0.4555,  ...,  0.2281, -0.8219],
                [ 0.2815, -0.0547,  ..., -0.4199,  0.5157]]),
            ...
        },
        'xmqSaQPzL1E': {
            ...
        },
        ...
    }

    Args:
        lfb_prefix_path (str): The storage path of lfb.
        max_num_sampled_feat (int): The max number of sampled features.
            Default: 5.
        window_size (int): Window size of sampling long term feature.
            Default: 60.
        lfb_channels (int): Number of the channels of the features stored
            in LFB. Default: 2048.
        dataset_modes (list | str): Load LFB of datasets with different modes,
            such as training, validation, testing datasets. If you don't do
            cross validation during training, just load the training set i.e.
            setting `dataset_modes = ['train']`. Default: ['train', 'val'].
        device (str): Where to load lfb. 'cpu' and 'gpu' are supported. If
            distributed training is used, this file will be loaded repeatly
            on RAM with `device = 'cpu'`. A 1.65GB half-precision ava lfb
            (including training and validation) occupies about 2GB GPU memory.
            Default: 'gpu'.
    """

    def __init__(self,
                 lfb_prefix_path,
                 max_num_sampled_feat=5,
                 window_size=60,
                 lfb_channels=2048,
                 dataset_modes=['train', 'val'],
                 device='gpu'):
        if not osp.exists(lfb_prefix_path):
            raise ValueError(
                f'lfb prefix path {lfb_prefix_path} does not exist!')
        self.lfb_prefix_path = lfb_prefix_path
        self.max_num_sampled_feat = max_num_sampled_feat
        self.window_size = window_size
        self.lfb_channels = lfb_channels
        if not isinstance(dataset_modes, list):
            assert isinstance(dataset_modes, str)
            dataset_modes = [dataset_modes]
        self.dataset_modes = copy.deepcopy(dataset_modes)
        self.device = device

        rank, world_size = get_dist_info()
        if self.device == 'gpu':
            map_location = f'cuda:{rank}'
        elif self.device == 'cpu':
            map_location = 'cpu'
        else:
            raise ValueError(
                f"device must be 'cpu' or 'gpu', but get {self.device}.")

        # Loading LFB from different lfb path.
        self.lfb = {}
        for dataset_mode in self.dataset_modes:
            lfb_path = osp.normpath(
                osp.join(lfb_prefix_path, f'lfb_{dataset_mode}.pkl'))
            print(f'Loading LFB from {lfb_path}...')
            self.lfb.update(torch.load(lfb_path, map_location=map_location))

        if self.device == 'gpu':
            print(f'LFB has been loaded on GPU {rank}.')
        elif self.device == 'cpu':
            print('LFB has been loaded on CPU.')

    def sample_long_term_features(self, video_id, timestamp):
        video_features = self.lfb[video_id]

        # Sample long term features.
        window_size, K = self.window_size, self.max_num_sampled_feat
        start = timestamp - (window_size // 2)
        lt_feats = torch.zeros(window_size * K, self.lfb_channels)

        for idx, sec in enumerate(range(start, start + window_size)):
            if sec in video_features:
                # `num_feat` is the number of roi features in this second.
                num_feat = len(video_features[sec])
                num_feat_sampled = min(num_feat, K)
                # Sample some roi features randomly.
                random_lfb_indices = np.random.choice(
                    range(num_feat), num_feat_sampled, replace=False)

                for k, rand_idx in enumerate(random_lfb_indices):
                    lt_feats[idx * K + k] = video_features[sec][rand_idx]

        # [window_size * max_num_sampled_feat, lfb_channels]
        return lt_feats

    def __getitem__(self, img_key):
        """Sample long term features like `lfb['0f39OWEqJ24,0902']` where `lfb`
        is a instance of class LFB."""
        video_id, timestamp = img_key.split(',')
        return self.sample_long_term_features(video_id, int(timestamp))

    def __len__(self):
        """The number of videos whose ROI features are stored in LFB."""
        return len(self.lfb)
